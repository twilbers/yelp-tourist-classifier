{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = pd.read_csv('data/all_reviews_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yelp_df.drop(['business_star_rating','business_zip','review_raiting', 'useful', 'funny', 'cool'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df[['business_star_rating', 'review_raiting', 'useful', 'funny', 'cool']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make sure every observation has a label\n",
    "yelp_df.query('label != \"local\" and label != \"remote\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure reviews do not end on whitespace (this would be indicative that the full review was no scraped)\n",
    "import re\n",
    "yelp_df[['review_text', 'business_url']].assign(end_on_whitespace=yelp_df['review_text'].apply(lambda x: re.search(r'[ \\f\\t\\v]+$', x) != None)).query('end_on_whitespace == True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.isna().sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Anaysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of labels per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yelp_df[['business_url']].groupby([yelp_df['business_state'], yelp_df['label'] ]).agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percerntage of labels per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_label_group = yelp_df[['business_state','label']].groupby(\n",
    "    [yelp_df['business_state'], yelp_df['label']])\n",
    "\n",
    "biz_label_group.agg('count').apply(lambda x: 100 * x / float(x.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yelp_df.groupby(yelp_df['label']).agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio of business rating to review rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df[['business_star_rating', 'review_raiting', 'label']].assign(review_biz_ratio= (yelp_df['review_raiting']/yelp_df['business_star_rating'])).groupby('label').agg(lambda x: x.sum()/x.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add tagged reviews to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('dump/scraped_reviews_tagged.p', 'rb') as f_reviews_tagged:\n",
    "    reviews_tagged = pickle.load(f_reviews_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df['reviews_tagged'] = reviews_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.rename(columns={'cool': 'cool_', 'label': 'label_', 'funny': 'funny_', 'useful': 'useful_'})\n",
    "\n",
    "# yelp_df[['reviewer_id']] = yelp_df[['reviewer_id']].drop_duplicates()\n",
    "# yelp_df = yelp_df[yelp_df['reviewer_id'].notnull()]\n",
    "\n",
    "yelp_df[['business_url']] = yelp_df[['business_url']].drop_duplicates(keep='first')\n",
    "yelp_df = yelp_df[yelp_df['business_url'].notnull()]\n",
    "\n",
    "state_min = min(yelp_df.query('business_state != \"NJ\"').groupby('business_state').agg('count').iloc[:, 0 ])\n",
    "\n",
    "sample_ny = yelp_df.query('business_state == \"NY\"').sample(n=state_min)\n",
    "sample_nv = yelp_df.query('business_state == \"NV\"').sample(n=state_min)\n",
    "sample_ca = yelp_df.query('business_state == \"CA\"').sample(n=state_min)\n",
    "sample_fl = yelp_df.query('business_state == \"FL\"').sample(n=state_min)\n",
    "sample_il = yelp_df.query('business_state == \"IL\"').sample(n=state_min)\n",
    "\n",
    "sample = pd.concat([sample_ny, sample_nv, sample_ca, sample_fl, sample_il]).reset_index(drop = True)\n",
    "\n",
    "sample_min = min(sample.groupby('label_').agg('count').iloc[:, 0])\n",
    "\n",
    "local_sample = sample.query('label_ == \"local\"').sample(n=sample_min)\n",
    "remote_sample = sample.query('label_ == \"remote\"').sample(n=sample_min)\n",
    "\n",
    "yelp_df = pd.concat([local_sample, remote_sample]).reset_index(drop = True)\n",
    "\n",
    "print(\"Sample size: \", len(yelp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.drop('reviews_tagged', axis= 1).astype(object).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df[['review_length', 'label_']].groupby('label_').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "docs       = vectorizer.fit_transform(yelp_df['review_text'].apply(lambda x: x.lower()))\n",
    "features   = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualizer = FreqDistVisualizer(n= 40, fontsize=25, features=features, orient='h', size=(1200, 800))\n",
    "visualizer.fit(docs)\n",
    "visualizer.ax.legend(loc=4)\n",
    "visualizer.set_title()\n",
    "# Set the title\n",
    "# Create the vocab, count, and hapaxes labels\n",
    "infolabel = \"vocab: {:,}\\nword tokens: {:,}\\nhapax: {:,}\".format(\n",
    "    visualizer.vocab_, visualizer.words_, visualizer.hapaxes_\n",
    ")\n",
    "\n",
    "visualizer.ax.text(0.68, 0.97, infolabel, position=(.75,.1), transform=visualizer.ax.transAxes,\n",
    "             fontsize=20, verticalalignment='bottom',\n",
    "             bbox={'boxstyle':'round', 'facecolor':'white', 'alpha':.8})\n",
    "\n",
    "# Set the legend and the grid\n",
    "plt.title('Frequency Distribution of Top {} tokens'.format(visualizer.N), fontsize=30)\n",
    "plt.yticks(size=15)\n",
    "plt.xticks(size=15)\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.show(visualizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipf's law states natural language corpus of utterances, the frequency of any word type is inversely proportional to its rank in the frequency table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So frequency of the word with rank n is proportional to 1/n. In other words, the most ranked word is around twice as common as the second ranked word, and a thousand times more common than the word with rank 1,000.\n",
    "\n",
    "We can check Zipf's Law for the scraped corpus of Yelp reviews by plotting the frequencies of the word types in rank order on a log-log graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "corpus_retokenized = tokenizer.tokenize(' '.join(yelp_df['review_text']).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = FreqDist(corpus_retokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = sorted(word_freq.items(), reverse=True, key=operator.itemgetter(1))\n",
    "\n",
    "for i in range(len(word_freq)):\n",
    "    word_freq[i] = (i, word_freq[i][1])\n",
    "\n",
    "refreq_df = pd.DataFrame.from_dict(dict(word_freq), orient='index')\n",
    "refreq_df = refreq_df.rename(columns={0: 'frequency'})\n",
    "refreq_df['count'] = refreq_df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "s = plt.scatter(refreq_df['count'], refreq_df['frequency'])\n",
    "s.axes.loglog(True)\n",
    "plt.title('Word Frequency by Frequency Rank', fontsize=25)\n",
    "plt.ylabel('Token Frequency', fontsize=25)\n",
    "plt.xlabel('Word Type Rank (by Frequency)', fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PArt of Speech Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pos_counter():\n",
    "\n",
    "    adverbs = [u'RB', u'RBR', u'RBS', u'RBS\\r', u'RB\\r', u'RBR\\r']\n",
    "    simple_past = [u'VBD', u'VBD\\r']\n",
    "    simple_present = [u'VBP', u'VBZ', u'VBP\\r', u'VPZ\\r']\n",
    "    past_participle = [u'VBN', u'VBN\\r']\n",
    "    modal = [u'MD', u'MD\\r']\n",
    "    pn = [u'NNP', u'NNPS', u'NNP\\r', u'NNPS\\r']\n",
    "    prep = [u'IN', u'IN\\r']\n",
    "    nn = [u'NN', u'NN\\r']\n",
    "    adj = [u'JJ', u'JJ\\r']\n",
    "    dt = [u'DT', u'DT\\r']\n",
    "\n",
    "    def count_pos(tagged_reviews, pos_list):\n",
    "        count = 0\n",
    "        for review in tagged_reviews:\n",
    "            try:\n",
    "                if review[1] in pos_list:\n",
    "                    count += 1\n",
    "            except:\n",
    "                pass\n",
    "        return float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yelp_df[\"adv count\"] = np.array([\n",
    "#         pos_counter.count_pos(review, pos_counter.adverbs)\n",
    "#         for review in yelp_df['reviews_tagged']])\n",
    "\n",
    "# yelp_df[\"past prog\"] = np.array([\n",
    "#     pos_counter.count_pos(review, pos_counter.past_participle)\n",
    "#     for review in yelp_df['reviews_tagged']])\n",
    "\n",
    "# yelp_df[\"simple future\"] = np.array([\n",
    "#     pos_counter.count_pos(review, pos_counter.modal)\n",
    "#     for review in yelp_df['reviews_tagged']])\n",
    "\n",
    "# yelp_df[\"simple past\"] = np.array([\n",
    "#     pos_counter.count_pos(review, pos_counter.simple_past)\n",
    "#     for review in yelp_df['reviews_tagged']])\n",
    "\n",
    "# yelp_df[\"simple present\"] = np.array([\n",
    "#     pos_counter.count_pos(review, pos_counter.simple_present)\n",
    "#     for review in yelp_df['reviews_tagged']])\n",
    "\n",
    "# yelp_df['porper name'] = np.array([\n",
    "#     pos_counter.count_pos(review, pos_counter.pn)\n",
    "#     for review in yelp_df['reviews_tagged']])\n",
    "\n",
    "# yelp_df['prep count'] = np.array([\n",
    "#     pos_counter.count_pos(review, pos_counter.prep)\n",
    "#     for review in yelp_df['reviews_tagged']])\n",
    "\n",
    "# yelp_df['nn count'] = np.array([\n",
    "#     pos_counter.count_pos(review, pos_counter.nn)\n",
    "#     for review in yelp_df['reviews_tagged']])\n",
    "\n",
    "# yelp_df['adj count'] = np.array([\n",
    "#     pos_counter.count_pos(review, pos_counter.adj)\n",
    "#     for review in yelp_df['reviews_tagged']])\n",
    "\n",
    "# yelp_df['det count'] = np.array([\n",
    "#     pos_counter.count_pos(review, pos_counter.dt)\n",
    "#     for review in yelp_df['reviews_tagged']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise POST comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pltme = yelp_df.loc[:, ['det count', 'adj count', 'nn count', 'prep count', 'porper name', \"simple present\", \"simple past\", \"simple future\", \"past prog\", 'adv count', 'label_']].groupby('label_').agg('sum').apply(lambda x: (x[0] - x[1]) / (x[0] + x[1]))\n",
    "\n",
    "xticks = pltme.index\n",
    "ins = np.arange(len(xticks))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Pairwise comparison of tags (across labels)', fontsize=25)\n",
    "plt.bar(ins, pltme.sort_values())\n",
    "plt.xticks(ins, xticks, rotation=45, fontsize=20)\n",
    "\n",
    "plt.yticks(size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "pos_list = []\n",
    "for review in yelp_df['reviews_tagged']:\n",
    "    pos_dict = {}\n",
    "    for item in review:\n",
    "        if type(item) == tuple:\n",
    "            if item[1] in pos_dict:\n",
    "                pos_dict[item[1]] += 1.0\n",
    "            else:\n",
    "                pos_dict[item[1]] = 1.0\n",
    "            if not item[1] in tags:\n",
    "                tags.append(item[1])\n",
    "    pos_list.append(pos_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pltme = yelp_df[['label_']].join(\n",
    "    pd.DataFrame.from_dict(pos_list), on=None, how='left', lsuffix='',\n",
    "    rsuffix='', sort=False).fillna(0).groupby('label_').agg('sum').apply(lambda x: (x[0] - x[1]) / (x[0] + x[1])).sort_values().transform(lambda x:((x-x.mean())/x.std()))\n",
    "\n",
    "#pltme = pltme[pltme.between(-5, -.3) | pltme.between(.3, 5)]\n",
    "\n",
    "xticks = pltme.index\n",
    "ins = np.arange(len(xticks))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Pairwise comparison of tags (across labels)', fontsize=25)\n",
    "plt.bar(ins, pltme.sort_values())\n",
    "plt.xticks(ins, xticks, rotation=45, fontsize=20)\n",
    "plt.yticks(size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltme = yelp_df[['label_']].join(\n",
    "    pd.DataFrame.from_dict(pos_list), on=None, how='left', lsuffix='',\n",
    "    rsuffix='', sort=False).groupby('label_').agg(\n",
    "    lambda x: sum(x>1)).apply(\n",
    "    lambda x: 1 - (min(x)/max(x)) if max(x) != 0 else 0).sort_values()#.transform(lambda x:((x-x.mean())/x.std()))\n",
    "\n",
    "xticks = pltme.index\n",
    "ins = np.arange(len(xticks))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Pairwise comparison of tags (across labels)', fontsize=25)\n",
    "plt.bar(ins, pltme.sort_values())\n",
    "plt.xticks(ins, xticks, rotation=45, fontsize=20)\n",
    "plt.yticks(size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.join(\n",
    "    pd.DataFrame.from_dict(pos_list)[['#','RBS', 'RBR', 'VBZ',\n",
    "                                      'VBD','VBP', 'EX', 'JJR', 'LS', 'MD',\n",
    "                                      'VBN', 'EX', '$']],\n",
    "    on=None, how='left', lsuffix='', rsuffix='', sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.join(\n",
    "    pd.DataFrame.from_dict(pos_list),\n",
    "    on=None, how='left', lsuffix='', rsuffix='', sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### review length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df= yelp_df.assign(review_length =\n",
    "    yelp_df['review_text'].apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### week of year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.assign(week_of_year =\n",
    "    yelp_df['review_date'].apply(lambda x: time.strptime(x, \"%m/%d/%Y\").tm_yday // 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.assign(day_of_week =\n",
    "    yelp_df['review_date'].apply(lambda x: time.strptime(x, \"%m/%d/%Y\").tm_wday))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### city mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.assign(city_mentioned = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecIn = np.vectorize(lambda a, b: a.lower() in b.lower() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.assign(city_mentioned = np.where(vecIn(yelp_df['business_city'].values, yelp_df['review_text'].values), 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reviewer state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yelp_df = yelp_df.assign(reviewer_state = yelp_df['reviewer_location'].astype(str).apply(lambda x: x[-2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliance keep words function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliance(words, local_words, remote_words, theta=.50):\n",
    "    \"\"\" saliance(data) takes a dataframe and returns a list of variables to keep\n",
    "    that meet a salience theta\n",
    "    \"\"\"\n",
    "    keep_words = []\n",
    "    for i in range(words.shape[1]):\n",
    "        normalizer = words[:, i].nnz\n",
    "        l_prob_sum = local_words[:, i].nnz / normalizer\n",
    "        r_prob_sum = remote_words[:, i].nnz / normalizer\n",
    "\n",
    "        min_ = min(r_prob_sum, l_prob_sum)\n",
    "        max_ = max(r_prob_sum, l_prob_sum)\n",
    "        if max_ != 0:\n",
    "                salience = (1 - (min_ / max_))\n",
    "        else:\n",
    "                salience = 0\n",
    "        if salience > theta and salience != 1:\n",
    "            keep_words.append(i)\n",
    "    return keep_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unigram_vec = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=None,\n",
    "    stop_words = 'english',\n",
    "    ngram_range=(1, 1),\n",
    "    #min_df = .001,\n",
    "    #max_df = .999,\n",
    "    preprocessor=None,\n",
    "    max_features=1000)\n",
    "\n",
    "unigram_fit = unigram_vect.fit_transform(yelp_df['review_text'])\n",
    "\n",
    "unigrams = pd.DataFrame(\n",
    "     unigram_fit.A, columns=unigram_vect.get_feature_names())\n",
    "\n",
    "unigrams = pd.concat([\n",
    "    unigrams.reset_index(drop=True), \n",
    "    yelp_df[['label_']]], axis=1)\n",
    "\n",
    "sali = unigrams.groupby('label_').agg(\n",
    "    lambda x: sum(x > 0)).apply(\n",
    "    lambda x: 1 - (min(x)/max(x)) if max(x) != 0 else 0)\n",
    "sali_type = unigrams.groupby('label_').agg(\n",
    "    lambda x: sum(x > 0)).apply(\n",
    "    lambda x: 'local' if x[0] > x[1] else 'remote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sali_table = pd.DataFrame(sali[sali < 1])\n",
    "sali_table['label'] = sali_type[sali < 1]\n",
    "sali_table.sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import features_basic\n",
    "from features_pos import pos_counter, get_pos_pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "import statsmodels.discrete.discrete_model as sm\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = yelp_df.drop([\n",
    "    'business_city', 'business_state', 'business_name',\n",
    "    'reviewer_location', 'business_url', 'review_date',\n",
    "    'reviewer_id', 'reviews_tagged', 'review_text', 'label_', 'funny_', 'cool_'], axis=1).fillna(0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(yelp_df['label_'])\n",
    "\n",
    "logit = sm.Logit(np.asarray(y), np.asarray(X))\n",
    "logit.fit_regularized()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('data/X.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# drop colinear variables and variables that make dataset biased\n",
    "# X = yelp_df.drop([\n",
    "#     'business_city', 'business_zip', 'business_state', 'business_name',\n",
    "#     'reviewer_location', 'business_url', 'review_date',\n",
    "#     'reviewer_id', 'reviews_tagged', 'funny_', 'cool_', ],   \n",
    "#     axis=1)\n",
    "\n",
    "# Only language model (drop all non-POS features then add N-gram Model)\n",
    "\n",
    "X = yelp_df.drop([\n",
    "    'business_city', 'business_zip', 'business_state', 'business_name',\n",
    "    'reviewer_location', 'business_url', 'review_date', 'reviewer_id',\n",
    "    'reviews_tagged', 'funny_', 'cool_', 'useful_', 'review_length',\n",
    "    'week_of_year', 'day_of_week', 'business_star_rating',\n",
    "    'review_raiting'],   \n",
    "    axis=1)\n",
    "\n",
    "print(\"Model feature space includes:\", ', '.join(X.columns))\n",
    "\n",
    "y = yelp_df[['label_']].astype('category')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, test_size=0.25)\n",
    "\n",
    "unigram_vect = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=None,\n",
    "    stop_words = None,\n",
    "    ngram_range=(1, 2),\n",
    "    #min_df = .001,\n",
    "    #max_df = .999,\n",
    "    preprocessor=None,\n",
    "    max_features=10000)\n",
    "\n",
    "unigram_fit = unigram_vect.fit_transform(X_train['review_text'])\n",
    "unigram_transform = unigram_vect.transform(X_test['review_text'])\n",
    "\n",
    "local_words = unigram_vect.transform(\n",
    "    X_train.query('label_ == \"local\"')['review_text'])\n",
    "remote_words = unigram_vect.transform(\n",
    "    X_train.query('label_ == \"remote\"')['review_text'])\n",
    "keep_index = saliance(unigram_fit, local_words, remote_words, theta=.5)\n",
    "\n",
    "unigram_transform = csr_matrix(unigram_transform[:, keep_index])\n",
    "unigram_fit = csr_matrix(unigram_fit[:, keep_index])\n",
    "keep_words = np.array(unigram_vect.get_feature_names())[keep_index]\n",
    "\n",
    "unigram_train = pd.DataFrame(unigram_fit.A, columns=keep_words)\n",
    "\n",
    "unigram_test = pd.DataFrame(unigram_transform.A, columns=keep_words)\n",
    "\n",
    "# unigram_train = pd.DataFrame(unigram_fit.A, columns=unigram_vect.get_feature_names())\n",
    "# unigram_test = pd.DataFrame(unigram_transform.A, columns=unigram_vect.get_feature_names())\n",
    "\n",
    "print(unigram_train.shape[1], \" n-grams in model\")\n",
    "print(unigram_train.columns)\n",
    "\n",
    "X_train = X_train.drop(['review_text', 'label_'], axis='columns')\n",
    "X_train = X_train.join(\n",
    "    unigram_train, on=None, how='left', lsuffix='',\n",
    "    rsuffix='', sort=False).fillna(0)\n",
    "\n",
    "X_test = X_test.drop(['review_text', 'label_'], axis='columns')\n",
    "X_test = X_test.join(\n",
    "    unigram_test, on=None, how='left', lsuffix='',\n",
    "    rsuffix='', sort=False).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ling = {'business_zip', 'day_of_week', 'week_of_year'}\n",
    "\n",
    "if non_ling <= set(X_train.columns):\n",
    "    business_zip_train = X_train[['business_zip']].fillna(0).astype(int)\n",
    "    business_zip_test = X_test[['business_zip']].fillna(0).astype(int)\n",
    "\n",
    "    day_train = X_train[['day_of_week']]\n",
    "    day_test = X_test[['day_of_week']]\n",
    "\n",
    "    week_train = X_train[['week_of_year']]\n",
    "    week_test = X_test[['week_of_year']]\n",
    "\n",
    "    cityref_train = X_train[['city_mentioned']]\n",
    "    cityref_test = X_test[['city_mentioned']]\n",
    "\n",
    "X_std = StandardScaler()\n",
    "train_values = X_std.fit_transform(X_train.values)\n",
    "test_values = X_std.transform(X_test.values)\n",
    "\n",
    "X_train = pd.DataFrame(train_values, index=X_train.index, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(test_values, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "if non_ling <= set(X_train.columns):\n",
    "    X_train[['business_zip']] = business_zip_train\n",
    "    X_test[['business_zip']] = business_zip_test\n",
    "\n",
    "    X_train[['day_of_week']] = day_train\n",
    "    X_test[['day_of_week']] = day_test\n",
    "\n",
    "    X_train[['week_of_year']] = week_train\n",
    "    X_test[['week_of_year']] = week_test\n",
    "\n",
    "    X_train[['city_mentioned']] = cityref_train\n",
    "    X_test[['city_mentioned']] = cityref_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = naive_bayes.MultinomialNB()\n",
    "nb_fit = nb.fit(X_train, y_train)\n",
    "score_nb = nb.score(X_test, y_test)\n",
    "\n",
    "y_pred_nb = nb_fit.predict(X_test)\n",
    "cnf_nb = confusion_matrix(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb_fit = gnb.fit(X_train, y_train.values.ravel())\n",
    "score_nb = gnb.score(X_test, y_test)\n",
    "\n",
    "y_pred_nb = gnb_fit.predict(X_test)\n",
    "cnf_nb = confusion_matrix(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='liblinear')\n",
    "logistic_fit = logistic.fit(X_train, y_train.values.ravel())\n",
    "score_lr = logistic.score(X_test, y_test)\n",
    "\n",
    "y_pred_lr = logistic_fit.predict(X_test)\n",
    "cnf_lr = confusion_matrix(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LR: \", score_lr, \"\\n\",\n",
    "      \"NB: \", score_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame({'feature': X_train.columns, 'coef': [x for y in logistic.coef_ for x in y]})\n",
    "d.sort_values(by='coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Decision Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision(y_label, y_pred):\n",
    "    dec = []\n",
    "    num = len(y_pred)\n",
    "    correct = np.array(y_label == y_pred)\n",
    "    for i in range(1,num):\n",
    "        dec.append((i/len(correct), sum(correct[:i]/i)))\n",
    "    return dec\n",
    "dec_lr = get_decision(y_test['label_'], y_pred_lr)\n",
    "dec_nb = get_decision(y_test['label_'], y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(*zip(*dec_lr[0:-1]), dashes=[3, 3])\n",
    "plt.plot(*zip(*dec_nb[0:-1]), dashes=[3, 3])\n",
    "plt.title('Accuracy by decision', fontsize=25)\n",
    "plt.legend(['Logistic Regression', 'Naive Bayes'], fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=25)\n",
    "plt.xlabel('Decision', fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=25)\n",
    "    plt.grid(False)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=25)\n",
    "    plt.yticks(tick_marks, classes, fontsize=25)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_nb)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = np.array(['remote', 'local'])\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix (without normalization)')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
