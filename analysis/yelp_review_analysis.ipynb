{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = pd.read_csv('/home/gavagai/all_reviews.csv')\n",
    "to_drop = yelp_df.query('label != \"remote\" and label != \"local\"').index\n",
    "yelp_df = yelp_df.drop(to_drop)\n",
    "yelp_df.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yelp_df.query('label != \"remote\" and label != \"local\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.to_csv('all_reviews_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = pd.read_csv('/home/gavagai/all_reviews_cleaned.csv')\n",
    "#yelp_df = yelp_df.drop('Unnamed: 0', axis=1)\n",
    "#yelp_df = yelp_df.drop_duplicates()\n",
    "#yelp_df = yelp_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "biz_df = yelp_df.iloc[:,0:6].drop_duplicates().astype('object')\n",
    "biz_df[['business_star_rating']] = biz_df[['business_star_rating']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "biz_df.drop('business_star_rating', axis = 1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biz_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_df[['business_star_rating', 'business_url']].groupby('business_star_rating').agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yelp_df.drop(['business_star_rating','business_zip','review_raiting', 'useful', 'funny', 'cool'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df[['business_star_rating', 'review_raiting', 'useful', 'funny', 'cool']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.isna().sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Anaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df[['business_url']].groupby([yelp_df['business_state'], yelp_df['label'] ]).agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_label_group = yelp_df[['business_state','label']].groupby(\n",
    "    [yelp_df['business_state'], yelp_df['label']])\n",
    "\n",
    "biz_label_group.agg('count').apply(lambda x: 100 * x / float(x.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yelp_df.groupby(yelp_df['label']).agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = yelp_df[['funny', 'label', 'business_url', 'review_text']].sort_values('funny', ascending=False).iloc[0,3]\n",
    "re.search(r'[ \\f\\t\\v]+$', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "yelp_df[['review_text', 'business_url']].assign(end_on_whitespace=yelp_df['review_text'].apply(lambda x: re.search(' +$', x) != None)).query('end_on_whitespace == True')\n",
    "#re.findall('\\s$', yelp_df['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yelp_df.isna().sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.sort_values('funny', ascending=False).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fretures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length of review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df[['review_text', 'label']].assign(review_length =\n",
    "    yelp_df['review_text'].apply(lambda x: len(x))).query('label == \"local\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df[['review_text', 'label']].assign(review_length =\n",
    "    yelp_df['review_text'].apply(lambda x: len(x))).query('label == \"remote\"').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ratio of business rating to review rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yelp_df[['business_star_rating', 'review_raiting']].assign(review_biz_ratio= (yelp_df['review_raiting']/yelp_df['business_star_rating'])).sort_values('review_biz_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "docs       = vectorizer.fit_transform(yelp_df['review_text'].apply(lambda x: x.lower()))\n",
    "features   = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = FreqDistVisualizer(n= 40, fontsize=25, features=features, orient='h', size=(1200, 800))\n",
    "visualizer.fit(docs)\n",
    "visualizer.ax.legend(loc=4)\n",
    "visualizer.set_title()\n",
    "# Set the title\n",
    "# Create the vocab, count, and hapaxes labels\n",
    "infolabel = \"vocab: {:,}\\nword tokens: {:,}\\nhapax: {:,}\".format(\n",
    "    visualizer.vocab_, visualizer.words_, visualizer.hapaxes_\n",
    ")\n",
    "\n",
    "visualizer.ax.text(0.68, 0.97, infolabel, position=(.75,.1), transform=visualizer.ax.transAxes,\n",
    "             fontsize=20, verticalalignment='bottom',\n",
    "             bbox={'boxstyle':'round', 'facecolor':'white', 'alpha':.8})\n",
    "\n",
    "# Set the legend and the grid\n",
    "plt.title('Frequency Distribution of Top {} tokens'.format(visualizer.N), fontsize=30)\n",
    "plt.yticks(size=15)\n",
    "plt.xticks(size=15)\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.show(visualizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "corpus_retokenized = tokenizer.tokenize(' '.join(yelp_df['review_text']).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reword_freq = FreqDist(corpus_retokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "refreq = reword_freq\n",
    "#freq = dict(sorted(freq.items(), reverse=True, key=lambda kv: kv[1]))\n",
    "refreq = sorted(refreq.items(), reverse=True, key=operator.itemgetter(1))\n",
    "for i in range(len(refreq)):\n",
    "    refreq[i] = (i, refreq[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refreq_df = pd.DataFrame.from_dict(dict(refreq), orient='index')\n",
    "refreq_df = refreq_df.rename(columns={0: 'frequency'})\n",
    "refreq_df['word type rank (by frequency)'] = refreq_df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "s = plt.scatter(refreq_df['count'], refreq_df['frequency'])\n",
    "s.axes.loglog(True)\n",
    "plt.title('Word Frequency by Frequency Rank', fontsize=25)\n",
    "plt.ylabel('Token Frequency', fontsize=25)\n",
    "plt.xlabel('Word Type Rank (by Frequency)', fontsize=25)\n",
    "#refreq_df.plot(kind='scatter', loglog=True,  x='count', y='frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipf's law states natural language corpus of utterances, the frequency of any word type is inversely proportional to its rank in the frequency table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So frequency of the word with rank n is proportional to 1/n. In other words, the most ranked word is around twice as common as the second ranked word, and a thousand times more common than the word with rank 100,000.)\n",
    "\n",
    "We can check Zipf's Law for the scraped corpus of Yelp reviews by plotting the frequencies of the word types in rank order on a log-log graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add confusion matrx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add decision Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = pd.read_csv('all_reviews_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_city</th>\n",
       "      <th>business_name</th>\n",
       "      <th>business_star_rating</th>\n",
       "      <th>business_url</th>\n",
       "      <th>business_zip</th>\n",
       "      <th>cool</th>\n",
       "      <th>funny</th>\n",
       "      <th>label</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_raiting</th>\n",
       "      <th>review_text</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_location</th>\n",
       "      <th>useful</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_state</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>6859</td>\n",
       "      <td>6859</td>\n",
       "      <td>6859</td>\n",
       "      <td>6859</td>\n",
       "      <td>6851</td>\n",
       "      <td>2833</td>\n",
       "      <td>2262</td>\n",
       "      <td>6859</td>\n",
       "      <td>6859</td>\n",
       "      <td>6859</td>\n",
       "      <td>6859</td>\n",
       "      <td>6859</td>\n",
       "      <td>6859</td>\n",
       "      <td>3912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL</th>\n",
       "      <td>12015</td>\n",
       "      <td>12015</td>\n",
       "      <td>12015</td>\n",
       "      <td>12015</td>\n",
       "      <td>12015</td>\n",
       "      <td>4264</td>\n",
       "      <td>3444</td>\n",
       "      <td>12015</td>\n",
       "      <td>12015</td>\n",
       "      <td>12015</td>\n",
       "      <td>12015</td>\n",
       "      <td>12015</td>\n",
       "      <td>12015</td>\n",
       "      <td>5865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL</th>\n",
       "      <td>15796</td>\n",
       "      <td>15796</td>\n",
       "      <td>15796</td>\n",
       "      <td>15796</td>\n",
       "      <td>15796</td>\n",
       "      <td>5570</td>\n",
       "      <td>4415</td>\n",
       "      <td>15796</td>\n",
       "      <td>15796</td>\n",
       "      <td>15796</td>\n",
       "      <td>15796</td>\n",
       "      <td>15796</td>\n",
       "      <td>15796</td>\n",
       "      <td>8390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NJ</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>97</td>\n",
       "      <td>69</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NV</th>\n",
       "      <td>9725</td>\n",
       "      <td>9725</td>\n",
       "      <td>9725</td>\n",
       "      <td>9725</td>\n",
       "      <td>9725</td>\n",
       "      <td>3593</td>\n",
       "      <td>2592</td>\n",
       "      <td>9725</td>\n",
       "      <td>9725</td>\n",
       "      <td>9725</td>\n",
       "      <td>9725</td>\n",
       "      <td>9725</td>\n",
       "      <td>9725</td>\n",
       "      <td>4866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NY</th>\n",
       "      <td>8662</td>\n",
       "      <td>8662</td>\n",
       "      <td>8662</td>\n",
       "      <td>8662</td>\n",
       "      <td>8662</td>\n",
       "      <td>3310</td>\n",
       "      <td>2271</td>\n",
       "      <td>8662</td>\n",
       "      <td>8662</td>\n",
       "      <td>8662</td>\n",
       "      <td>8662</td>\n",
       "      <td>8662</td>\n",
       "      <td>8662</td>\n",
       "      <td>4746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_city  business_name  business_star_rating  \\\n",
       "business_state                                                       \n",
       "CA                       6859           6859                  6859   \n",
       "FL                      12015          12015                 12015   \n",
       "IL                      15796          15796                 15796   \n",
       "NJ                        300            300                   300   \n",
       "NV                       9725           9725                  9725   \n",
       "NY                       8662           8662                  8662   \n",
       "\n",
       "                business_url  business_zip  cool  funny  label  review_date  \\\n",
       "business_state                                                                \n",
       "CA                      6859          6851  2833   2262   6859         6859   \n",
       "FL                     12015         12015  4264   3444  12015        12015   \n",
       "IL                     15796         15796  5570   4415  15796        15796   \n",
       "NJ                       300           300    97     69    300          300   \n",
       "NV                      9725          9725  3593   2592   9725         9725   \n",
       "NY                      8662          8662  3310   2271   8662         8662   \n",
       "\n",
       "                review_raiting  review_text  reviewer_id  reviewer_location  \\\n",
       "business_state                                                                \n",
       "CA                        6859         6859         6859               6859   \n",
       "FL                       12015        12015        12015              12015   \n",
       "IL                       15796        15796        15796              15796   \n",
       "NJ                         300          300          300                300   \n",
       "NV                        9725         9725         9725               9725   \n",
       "NY                        8662         8662         8662               8662   \n",
       "\n",
       "                useful  \n",
       "business_state          \n",
       "CA                3912  \n",
       "FL                5865  \n",
       "IL                8390  \n",
       "NJ                 155  \n",
       "NV                4866  \n",
       "NY                4746  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df.groupby('business_state').agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### review length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df= yelp_df.assign(review_length =\n",
    "    yelp_df['review_text'].apply(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### week of year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.assign(week_of_year =\n",
    "    yelp_df['review_date'].apply(lambda x: time.strptime(x, \"%m/%d/%Y\").tm_yday // 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.assign(day_of_week =\n",
    "    yelp_df['review_date'].apply(lambda x: time.strptime(x, \"%m/%d/%Y\").tm_wday))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### city mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.assign(city_mentioned = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecIn = np.vectorize(lambda a, b: a.lower() in b.lower() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.assign(city_mentioned = np.where(vecIn(yelp_df['business_city'].values, yelp_df['review_text'].values), 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df['business_city'].values in yelp_df['review_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53327</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53328</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53329</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53330</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53331</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53332</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53333</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53334</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53335</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53336</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53337</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53338</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53339</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53340</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53341</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53342</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53343</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53344</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53345</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53346</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53347</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53348</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53349</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53350</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53351</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53352</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53353</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53354</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53355</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53356</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53357 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       city_mentioned\n",
       "0                   0\n",
       "1                   1\n",
       "2                   0\n",
       "3                   0\n",
       "4                   0\n",
       "5                   0\n",
       "6                   0\n",
       "7                   0\n",
       "8                   0\n",
       "9                   0\n",
       "10                  0\n",
       "11                  0\n",
       "12                  0\n",
       "13                  0\n",
       "14                  0\n",
       "15                  0\n",
       "16                  0\n",
       "17                  0\n",
       "18                  0\n",
       "19                  0\n",
       "20                  0\n",
       "21                  0\n",
       "22                  0\n",
       "23                  0\n",
       "24                  1\n",
       "25                  0\n",
       "26                  0\n",
       "27                  0\n",
       "28                  0\n",
       "29                  0\n",
       "...               ...\n",
       "53327               0\n",
       "53328               0\n",
       "53329               0\n",
       "53330               0\n",
       "53331               0\n",
       "53332               0\n",
       "53333               0\n",
       "53334               0\n",
       "53335               0\n",
       "53336               0\n",
       "53337               0\n",
       "53338               0\n",
       "53339               0\n",
       "53340               0\n",
       "53341               0\n",
       "53342               0\n",
       "53343               0\n",
       "53344               0\n",
       "53345               0\n",
       "53346               0\n",
       "53347               0\n",
       "53348               0\n",
       "53349               0\n",
       "53350               0\n",
       "53351               0\n",
       "53352               0\n",
       "53353               0\n",
       "53354               0\n",
       "53355               0\n",
       "53356               0\n",
       "\n",
       "[53357 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_df[['city_mentioned']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reviewer state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = yelp_df.assign(reviewer_state = yelp_df['reviewer_location'].astype(str).apply(lambda x: x[-2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk import word_tokenize\n",
    "from os import environ\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "def review_tokenize(reviews):\n",
    "    return map(lambda review: word_tokenize(review), reviews)\n",
    "\n",
    "def review_tager(tokenized_reviews):    \n",
    "    st_model_path = r'SPOST/models/english-bidirectional-distsim.tagger'\n",
    "    st = StanfordPOSTagger(st_model_path,\n",
    "                           r'SPOST/stanford-postagger.jar')\n",
    "    results = []\n",
    "    errors = []\n",
    "    count = 0\n",
    "\n",
    "#     return map(lambda review: st.tag(review), tokenized_reviews)\n",
    "    for review in tokenized_reviews:\n",
    "        try:\n",
    "            results.append(st.tag(review))\n",
    "            count += 1\n",
    "        except:\n",
    "            print(count)\n",
    "            errors.append(count)\n",
    "            results.append(review)\n",
    "            count += 1\n",
    "    print('errors for the following indexes\\n', errors)\n",
    "    return results  # [st.tag(review) for review in tokenized_reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_reviews = review_tokenize(yelp_df['review_text'])\n",
    "# reviews_tagged = review_tager(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reviews_tagged.p', 'rb') as f_reviews_tagged:\n",
    "    reviews_tagged = pickle.load(f_reviews_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df['reviews_tagged'] = reviews_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pos_counter():\n",
    "\n",
    "    adverbs = [u'RB', u'RBR', u'RBS', u'RBS\\r', u'RB\\r', u'RBR\\r']\n",
    "    simple_past = [u'VBD', u'VBD\\r']\n",
    "    simple_present = [u'VBP', u'VPZ', u'VBP\\r', u'VPZ\\r']\n",
    "    past_participle = [u'VBN', u'VBN\\r']\n",
    "    modal = [u'MD', u'MD\\r']\n",
    "    pn = [u'NNP', u'NNPS', u'NNP\\r', u'NNPS\\r']\n",
    "    prep = [u'IN', u'IN\\r']\n",
    "    nn = [u'NN', u'NN\\r']\n",
    "    adj = [u'JJ', u'JJ\\r']\n",
    "    dt = [u'DT', u'DT\\r']\n",
    "\n",
    "    def count_pos(tagged_reviews, pos_list):\n",
    "        count = 0\n",
    "        for review in tagged_reviews:\n",
    "            try:\n",
    "                if review[1] in pos_list:\n",
    "                    count += 1\n",
    "            except:\n",
    "                pass\n",
    "        return float(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df[\"adv count\"] = [\n",
    "        pos_counter.count_pos(review, pos_counter.adverbs)\n",
    "        for review in reviews_tagged]\n",
    "\n",
    "yelp_df[\"past prog\"] = [\n",
    "    pos_counter.count_pos(review, pos_counter.past_participle)\n",
    "    for review in reviews_tagged]\n",
    "\n",
    "yelp_df[\"simple future\"] = [\n",
    "    pos_counter.count_pos(review, pos_counter.modal)\n",
    "    for review in reviews_tagged]\n",
    "\n",
    "yelp_df[\"simple past\"] = [\n",
    "    pos_counter.count_pos(review, pos_counter.simple_past)\n",
    "    for review in reviews_tagged]\n",
    "\n",
    "yelp_df[\"simple present\"] = [\n",
    "    pos_counter.count_pos(review, pos_counter.simple_present)\n",
    "    for review in reviews_tagged]\n",
    "\n",
    "yelp_df['porper name'] = [\n",
    "    pos_counter.count_pos(review, pos_counter.pn)\n",
    "    for review in reviews_tagged]\n",
    "\n",
    "yelp_df['prep count'] = [\n",
    "    pos_counter.count_pos(review, pos_counter.prep)\n",
    "    for review in reviews_tagged]\n",
    "\n",
    "yelp_df['nn count'] = [\n",
    "    pos_counter.count_pos(review, pos_counter.nn)\n",
    "    for review in reviews_tagged]\n",
    "\n",
    "yelp_df['adj count'] = [\n",
    "    pos_counter.count_pos(review, pos_counter.adj)\n",
    "    for review in reviews_tagged]\n",
    "\n",
    "yelp_df['det count'] = [\n",
    "    pos_counter.count_pos(review, pos_counter.dt)\n",
    "    for review in reviews_tagged]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliance(unigrams, unigram_labels, theta=.50):\n",
    "    \"\"\" saliance(data) takes a dataframe and returns a list of dropable variables\n",
    "    that do not meet a salience theta\n",
    "    \"\"\"\n",
    "    unigrams = pd.concat([unigrams.reset_index(drop=True), unigram_labels], axis=1)\n",
    "    unigrams_l = unigrams.query('label == \"local\"')\n",
    "    unigrams_r = unigrams.query('label == \"remote\"')\n",
    "\n",
    "    drop_words = []\n",
    "    \n",
    "    for word in unigrams.drop('label', axis = 1):\n",
    "        normalizer = len([x for x in unigrams[word] if x > 0])\n",
    "        l_prob_sum = len([x for x in unigrams_l[word] if x > 0]) / normalizer\n",
    "        r_prob_sum =  len([x for x in unigrams_r[word] if x > 0]) / normalizer\n",
    "        min_ = min(r_prob_sum, l_prob_sum)\n",
    "        max_ = max(r_prob_sum, l_prob_sum)\n",
    "        if max_ != 0:\n",
    "            salience = (1 - (min_/max_))\n",
    "        else:\n",
    "            salience = 0\n",
    "        if salience == 1 or salience < theta:\n",
    "            drop_words.append(word)\n",
    "    return drop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram_vect = sklearn.feature_extraction.text.CountVectorizer(\n",
    "#     analyzer=\"word\",\n",
    "#     tokenizer=None,\n",
    "#     preprocessor=None,\n",
    "#     stop_words=None,\n",
    "#     max_features=1000)\n",
    "\n",
    "# unigram_fit = unigram_vect.fit_transform(yelp_df['review_text'])\n",
    "\n",
    "# unigrams = pd.DataFrame(\n",
    "#      unigram_fit.A, columns=unigram_vect.get_feature_names())\n",
    "\n",
    "\n",
    "# unigrams = pd.concat([unigrams.reset_index(drop=True), yelp_df['label']], axis=1)\n",
    "# unigrams_l = unigrams.query('label == \"local\"')\n",
    "# unigrams_r = unigrams.query('label == \"remote\"')\n",
    "\n",
    "# def map_saliance(word, theta = .6):\n",
    "#     \"\"\" saliance(data) takes a dataframe and returns a list of dropable variables\n",
    "#     that do not meet a salience theta\n",
    "#     \"\"\"\n",
    "#     normalizer = len([x for x in unigrams[word] if x > 0])\n",
    "#     l_prob_sum = len([x for x in unigrams_l[word] if x > 0]) / normalizer\n",
    "#     r_prob_sum =  len([x for x in unigrams_r[word] if x > 0]) / normalizer\n",
    "#     min_ = min(r_prob_sum, l_prob_sum)\n",
    "#     max_ = max(r_prob_sum, l_prob_sum)\n",
    "#     if max_ != 0:\n",
    "#         salience = (1 - (min_/max_))\n",
    "#     else:\n",
    "#         salience = 0\n",
    "#     if salience > theta and max(r_prob_sum, l_prob_sum) == l_prob_sum:\n",
    "#             return (word, salience, 'local')\n",
    "#     elif salience > theta:\n",
    "#         (word, salience, 'remote')\n",
    "# map_saliance('10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4f0ebecf002b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m unigrams = pd.concat([\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0munigrams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     yelp_df[['label_']]], axis=1)\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/data_science/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill)\u001b[0m\n\u001b[1;32m   4071\u001b[0m             \u001b[0mnew_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4073\u001b[0;31m             \u001b[0mnew_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4075\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_casted_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/data_science/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   5112\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5113\u001b[0m         \"\"\"\n\u001b[0;32m-> 5114\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/data_science/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep, mgr)\u001b[0m\n\u001b[1;32m   3918\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3919\u001b[0m         return self.apply('copy', axes=new_axes, deep=deep,\n\u001b[0;32m-> 3920\u001b[0;31m                           do_integrity_check=False)\n\u001b[0m\u001b[1;32m   3921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/data_science/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3580\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3581\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3582\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/data_science/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep, mgr)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "unigram_vect = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=None,\n",
    "    preprocessor=None,\n",
    "    stop_words=None,\n",
    "    max_features=30000)\n",
    "\n",
    "yelp_df = yelp_df.rename(columns={'cool': 'cool_', 'label': 'label_', 'funny': 'funny_', 'useful': 'useful_'})\n",
    "\n",
    "unigram_fit = unigram_vect.fit_transform(yelp_df['review_text'])\n",
    "\n",
    "unigrams = pd.DataFrame(\n",
    "     unigram_fit.A, columns=unigram_vect.get_feature_names())\n",
    "\n",
    "unigrams = pd.concat([\n",
    "    unigrams.reset_index(drop=True), \n",
    "    yelp_df[['label_']]], axis=1)\n",
    "\n",
    "sali = unigrams.groupby('label_').agg(\n",
    "    lambda x: sum(x > 1)).apply(\n",
    "    lambda x: 1 - (min(x)/max(x)) if max(x) != 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sali_type = unigrams.groupby('label_').agg(\n",
    "    lambda x: sum(x > 1)).apply(\n",
    "    lambda x: 'local' if x[0] > x[1] else 'remote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l[l > .50][l != 1].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def saliance(words, local_words, remote_words, theta=.50):\n",
    "#     drop_words = []\n",
    "#     for i in range(words.shape[1]):\n",
    "#         normalizer = words[:, i].sum()\n",
    "#         l_prob_sum = local_words[:, i].sum() / normalizer\n",
    "#         r_prob_sum = remote_words[:, i].sum() / normalizer\n",
    "\n",
    "#         min_ = min(r_prob_sum, l_prob_sum)\n",
    "#         max_ = max(r_prob_sum, l_prob_sum)\n",
    "#         if max_ != 0:\n",
    "#                 salience = (1 - (min_/max_))\n",
    "#         else:\n",
    "#                 salience = 0\n",
    "#         if salience < theta:\n",
    "#             drop_words.append(i)\n",
    "#     return drop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "unigram_vect = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=None,\n",
    "    preprocessor=None,\n",
    "    stop_words=None,\n",
    "    max_features=50000)\n",
    "\n",
    "unigram_fit = unigram_vect.fit_transform(yelp_df['review_text'])\n",
    "\n",
    "unigram_model = pd.DataFrame(\n",
    "     unigram_fit.A, columns=unigram_vect.get_feature_names())\n",
    "\n",
    "\n",
    "def rank_saliance(unigrams, unigram_labels, theta=.50):\n",
    "\n",
    "    saliance_rank = {}\n",
    "\n",
    "    unigrams = pd.concat([unigrams.reset_index(drop=True), unigram_labels], axis=1)\n",
    "    unigrams_l = unigrams.query('label == \"local\"')\n",
    "    unigrams_r = unigrams.query('label == \"remote\"')\n",
    "\n",
    "    for word in unigrams.drop('label', axis = 1):\n",
    "        #normalizer = sum(unigrams[word])                 \n",
    "        normalizer = len([x for x in unigrams[word] if x > 0])\n",
    "        l_prob_sum = len([x for x in unigrams_l[word] if x > 0]) / normalizer\n",
    "        r_prob_sum =  len([x for x in unigrams_r[word] if x > 0]) / normalizer\n",
    "        min_ = min(r_prob_sum, l_prob_sum)\n",
    "        max_ = max(r_prob_sum, l_prob_sum)\n",
    "        if max_ != 0:\n",
    "            salience = (1 - (min_/max_))\n",
    "        else:\n",
    "            salience = 0\n",
    "        if salience > theta and max(r_prob_sum, l_prob_sum) == l_prob_sum:\n",
    "            saliance_rank[word] = (word, salience, 'local')\n",
    "        elif salience > theta:\n",
    "            saliance_rank[word] = (salience, 'remote')\n",
    "    return saliance_rank\n",
    "r = rank_saliance(unigram_model, yelp_df['label'], theta=.65)\n",
    "print(dict(sorted(r.items(), reverse=True, key=lambda kv: kv[1])))\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from Date Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df = pd.read_csv('all_reviews_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6859\n",
      "16484\n"
     ]
    }
   ],
   "source": [
    "yelp_df = yelp_df.rename(columns={'cool': 'cool_', 'label': 'label_', 'funny': 'funny_', 'useful': 'useful_'})\n",
    "\n",
    "state_min = min(yelp_df.query('business_state != \"NJ\"').groupby('business_state').agg('count').iloc[:, 0 ])\n",
    "\n",
    "print(state_min)\n",
    "\n",
    "sample_ny = yelp_df.query('business_state == \"NY\"').sample(n=state_min)\n",
    "sample_nv = yelp_df.query('business_state == \"NV\"').sample(n=state_min)\n",
    "sample_ca = yelp_df.query('business_state == \"CA\"').sample(n=state_min)\n",
    "sample_fl = yelp_df.query('business_state == \"FL\"').sample(n=state_min)\n",
    "sample_il = yelp_df.query('business_state == \"IL\"').sample(n=state_min)\n",
    "\n",
    "sample = pd.concat([sample_ny, sample_nv, sample_ca, sample_fl, sample_il]).reset_index(drop = True)\n",
    "\n",
    "sample_min = min(sample.groupby('label_').agg('count').iloc[:, 0])\n",
    "\n",
    "print(sample_min)\n",
    "\n",
    "local_sample = sample.query('label_ == \"local\"').sample(n=sample_min)\n",
    "remote_sample = sample.query('label_ == \"remote\"').sample(n=sample_min)\n",
    "\n",
    "yelp_df = pd.concat([local_sample, remote_sample]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_state = LabelEncoder()\n",
    "yelp_df[['business_city']] = le_state.fit_transform(yelp_df['business_city'])\n",
    "le_state = LabelEncoder()\n",
    "yelp_df[['business_state']] = le_state.fit_transform(yelp_df['business_state'])\n",
    "le_zip = LabelEncoder()\n",
    "yelp_df[['business_zip']] = le_zip.fit_transform(yelp_df['business_zip'])\n",
    "le_loc = LabelEncoder()\n",
    "yelp_df[['reviewer_location']] = le_loc.fit_transform(yelp_df['reviewer_location'])\n",
    "le_rstate = LabelEncoder()\n",
    "yelp_df[['reviewer_state']] = le_rstate.fit_transform(yelp_df['reviewer_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_df.drop(['business_city', 'business_state', 'reviews_tagged','business_name','business_url','review_date','reviewer_id'], axis = 1).to_csv('X.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue' '15' 'try']\n"
     ]
    }
   ],
   "source": [
    "X = yelp_df.drop(['business_city', 'business_state', 'business_name','business_url',\n",
    "                  'review_date','reviewer_id'], axis = 1)\n",
    "\n",
    "# X = yelp_df[['business_star_rating', 'business_zip', 'cool_', \n",
    "#          'funny_', 'review_raiting', 'useful_', 'review_length', \n",
    "#          'week_of_year', 'city_mentioned', 'adv count', 'past prog', \n",
    "#          'simple past', 'porper name', 'prep count', 'review_text', 'label' ]]\n",
    "\n",
    "y = yelp_df[['label']].astype('category')\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    X, y, train_size=0.75, test_size=0.25)\n",
    "\n",
    "unigram_vect = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=None,\n",
    "    preprocessor=None,\n",
    "    stop_words=None,\n",
    "    max_features=1000)\n",
    "\n",
    "unigram_fit = unigram_vect.fit_transform(X_train['review_text'])\n",
    "unigram_transform = unigram_vect.transform(X_test['review_text'])\n",
    "\n",
    "\n",
    "# def saliance(words, local_words, remote_words, theta=.50):\n",
    "#     drop_words = []\n",
    "#     for i in range(words.shape[1]):\n",
    "#         normalizer = words[:, i].sum()\n",
    "#         l_prob_sum = local_words[:, i].sum() / normalizer\n",
    "#         r_prob_sum = remote_words[:, i].sum() / normalizer\n",
    "\n",
    "#         min_ = min(r_prob_sum, l_prob_sum)\n",
    "#         max_ = max(r_prob_sum, l_prob_sum)\n",
    "#         if max_ != 0:\n",
    "#                 salience = (1 - (min_/max_))\n",
    "#         else:\n",
    "#                 salience = 0\n",
    "#         if salience < theta:\n",
    "#             drop_words.append(i)\n",
    "#     return drop_words\n",
    "\n",
    "\n",
    "print(np.array(unigram_vect.get_feature_names())[[100,4,900]])\n",
    "\n",
    "\n",
    "# unigram_train = pd.DataFrame(\n",
    "#      unigram_fit.A, columns=unigram_vect.get_feature_names())\n",
    "\n",
    "# unigram_test = pd.DataFrame(\n",
    "#      unigram_transform.A, columns=unigram_vect.get_feature_names())\n",
    "\n",
    "# local_words = unigram_vect.transform(X_train.query('label == \"local\"')['review_text'])\n",
    "# remote_words = unigram_vect.transform(X_train.query('label == \"remote\"')['review_text'])\n",
    "# drop_index = saliance(unigram_fit, local_words, remote_words, theta=.65)\n",
    "# drop_words = unigram_train.columns[drop_index]\n",
    "\n",
    "# unigram_labels = y_train\n",
    "# drop_words = saliance(unigram_train, unigram_labels, theta=.65)\n",
    "\n",
    "# unigram_train.drop(drop_words, axis=1, inplace=True)\n",
    "# unigram_test.drop(drop_words, axis=1, inplace=True)\n",
    "\n",
    "# print(unigram_train.shape[1], \" n-grams in model\")\n",
    "\n",
    "# X_train = X_train.drop(['review_text', 'label'], axis='columns')\n",
    "# X_train = X_train.join(unigram_train,\n",
    "#     on=None, how='left', lsuffix='', rsuffix='', sort=False)\n",
    "\n",
    "# X_test = X_test.drop(['review_text', 'label'], axis='columns')\n",
    "# X_test = X_test.join(unigram_test,\n",
    "#     on=None, how='left', lsuffix='', rsuffix='', sort=False)\n",
    "\n",
    "# X_train = X_train.fillna(0)\n",
    "# X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 1],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.sparse.csr.csr_matrix(unigram_fit)[:,[1,2,3]].A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LassoLarsIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression()\n",
    "logistic_fit = logistic.fit(X_train, y_train)\n",
    "y_pred = logistic_fit.predict(X_test)\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "logistic.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = []\n",
    "num = len(y_pred)\n",
    "correct = np.array(y_test['label'] == y_pred)\n",
    "for i in range(1,num):\n",
    "    dec.append((i/len(correct), sum(correct[:i]/i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(*zip(*dec[0:-1:100]), dashes=[3, 3])\n",
    "plt.title('Word Frequency by Frequency Rank', fontsize=25)\n",
    "plt.ylabel('Accuracy', fontsize=25)\n",
    "plt.xlabel('Decision', fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm = sklearn.svm.LinearSVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=25)\n",
    "    plt.grid(False)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, fontsize=25)\n",
    "    plt.yticks(tick_marks, classes, fontsize=25)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = np.array(['remote', 'local'])\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix (without normalization)')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
